# Stage 2: Note sequence generation model configuration
vocab_size: 167  # Must match tokenizer.VOCAB_SIZE
d_model: 512
nhead: 8
num_layers: 8
dim_feedforward: 2048
num_difficulties: 5
num_genres: 11
dropout: 0.1

# Training
learning_rate: 0.0003
weight_decay: 0.01
max_seq_length: 64  # Max tokens per onset timestamp
label_smoothing: 0.1
freeze_encoder: false

# Dataset
context_frames: 128  # Mel frames around each onset for audio context

# Inference
beam_size: 8
temperature: 1.0
top_p: 0.9  # Nucleus sampling threshold
