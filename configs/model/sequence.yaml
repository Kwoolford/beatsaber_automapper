# @package model.sequence
# Stage 2: Note sequence generation model configuration
vocab_size: 167  # Must match tokenizer.VOCAB_SIZE
d_model: 512
nhead: 8
num_layers: 8
dim_feedforward: 2048
num_difficulties: 5
num_genres: 1  # Disabled — 100% of maps have genre="unknown"
dropout: 0.1

# Conditioning dropout for Classifier-Free Guidance
conditioning_dropout: 0.2  # 20% dropout on difficulty/genre embeddings during training

# Training
learning_rate: 0.0003
weight_decay: 0.01
max_seq_length: 64  # Max tokens per onset timestamp
label_smoothing: 0.1
rhythm_weight: 3.0  # 3x weight on timing-sensitive tokens (EVENT_TYPE, SEP)
eos_weight: 1.0  # Normal weight — training data has no empty onsets (preprocessing filters them)
freeze_encoder: false

# Dataset
context_frames: 256  # Mel frames around each onset (~3 seconds of audio context)
# NOTE: 512 was 16x compute cost due to O(T²) encoder self-attention with no CNN downsampling.
# 256 gives 3 seconds (~16 beats at 120 BPM), 4x cost instead of 16x.

# Inter-onset context: feed previous K onset token sequences to break mode collapse
prev_context_k: 8  # Number of previous onset sequences (0 = disabled)

# Flow-aware auxiliary loss
flow_loss_alpha: 0.1  # Weight for flow parity auxiliary loss

# Difficulty filtering (null = all difficulties)
difficulties:
  - Expert
  - ExpertPlus

# Inference
beam_size: 8
temperature: 1.0
top_p: 0.9  # Nucleus sampling threshold
min_length: 7  # Min tokens before EOS: BOS + NOTE + COLOR + COL + ROW + DIR + ANGLE = 7
